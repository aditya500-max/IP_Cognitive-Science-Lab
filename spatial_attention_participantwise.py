# -*- coding: utf-8 -*-
"""Spatial_attention_participantWise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/109L0UJ2QgAXIYgGh3SPVJZegBhxGMqQv
"""



####### This code will help to mount Google Drive with this Notebook

from google.colab import drive
drive.mount('/content/drive')
import os
import pandas as pd
import csv
import numpy as np

"""## **Preprocessing: Everytime you data you need to run this below code**"""

# Define the base path
base_path = '/content/drive/MyDrive/Research/Spatial_attnetion&Emotion/Data/Pilot_data_main(ninth)'

# Define the list of experiments
experiments = ['E1', 'E2']

# Iterate over each participant
for participant_name in os.listdir(base_path):
    participant_path = os.path.join(base_path, participant_name)

    # Iterate over each experiment
    for experiment in experiments:
        # Read SYA.csv
        syacsv_path = os.path.join(participant_path, experiment, participant_name + '.csv')
        syacsv_data = pd.read_csv(syacsv_path)

        # Filter out the first 5 images
        filtered_syacsv_data = syacsv_data.iloc[6:]

        # Filter out rows with missing 'polygon_2.started' or 'ISI.stopped'
        filtered_syacsv_data = filtered_syacsv_data.dropna(subset=['main_bg.started', 'Main_ISI1_BG.stopped'])

        # Get the list of image names with valid 'polygon_2.started' and 'ISI.stopped'
        valid_image_names = filtered_syacsv_data['Emotional_images'].tolist()

        # Filter out rows with image names not in valid_image_names
        filtered_syacsv_data = filtered_syacsv_data[filtered_syacsv_data['Emotional_images'].isin(valid_image_names)]

        # Round the starting and ending times to double precision
        filtered_syacsv_data['main_bg.started'] = filtered_syacsv_data['main_bg.started'].round(2)
        filtered_syacsv_data['Main_ISI1_BG.stopped'] = filtered_syacsv_data['Main_ISI1_BG.stopped'].round(2)

        # Read mono.csv
        monocsv_path = os.path.join(participant_path, experiment, 'mono.csv')
        monocsv_data = pd.read_csv(monocsv_path)

        # Round the logged_time to double precision
        monocsv_data['logged_time'] = monocsv_data['logged_time'].round(2)

        # Initialize the final dataframe
        final_data = pd.DataFrame(columns=['image_name'])

        # Iterate over each image in SYA.csv
        for index, row in filtered_syacsv_data.iterrows():
            image_name = row['Emotional_images']
            start_time = row['main_bg.started']
            end_time = row['Main_ISI1_BG.stopped']
            response_key = row['main_exp_response.keys']
            #response_time = row['main_response_2.rt']
            location=row['Locations']
            cue_location=row['Location_cue']
            opposite_images=row['opposite_images']

            # Filter monocsv_data based on start_time and end_time
            filtered_monocsv_data = monocsv_data.loc[(monocsv_data['logged_time'] >= start_time) & (monocsv_data['logged_time'] <= end_time)]

            # Create a new row for the final dataframe
            #new_row = {'image_name': image_name, 'response_key': response_key, 'response_time': response_time,'location':location,'cue_location':cue_location,'opposite_images':opposite_images}
            new_row = {'image_name': image_name, 'response_key': response_key,'location':location,'cue_location':cue_location,'opposite_images':opposite_images}


            # Add pupil_measure2 values as columns in the new row
            for i, measure in enumerate(filtered_monocsv_data['pupil_measure2']):
                new_row[f't{i+1}'] = measure

            # Append the new row to the final dataframe
            final_data = pd.concat([final_data, pd.DataFrame([new_row])], ignore_index=True)


        # Save the filtered data as 'filtered_data.csv'
        filtered_data_output_path = os.path.join(participant_path, experiment, 'filtered_data.csv')
        filtered_syacsv_data.to_csv(filtered_data_output_path, index=False)

        # Save the final dataframe as a CSV file
        output_path = os.path.join(participant_path, experiment, 'raw_data_pupil.csv')
        final_data.to_csv(output_path, index=False)

################### Divide the raw data into correct and incorrect data
#############################################################################################
#################################################################################################
###############################################################################################


def process_responses(folder_path, folder_name):
    image_extension = ['.png', '.PNG']
    subfolders = ['E1', 'E2']

    for subfolder in subfolders:
        subfolder_path = os.path.join(folder_path, folder_name, subfolder)

        if os.path.exists(os.path.join(subfolder_path, 'raw_data_pupil.csv')):
            correct_responses = []
            incorrect_responses = []
            pupil_data_counter = 1

            # Read the correct_responses.csv file
            with open(os.path.join(subfolder_path, 'raw_data_pupil.csv'), 'r') as csv_file:
                csv_reader = csv.reader(csv_file)
                header = next(csv_reader)
                image_name_index = header.index('image_name')
                opposite_images_index=header.index('opposite_images')
                response_key_index = header.index('response_key')

                for row in csv_reader:
                    image_name = row[image_name_index]
                    opposite_images=row[opposite_images_index]
                    response_key = row[response_key_index]

                    if image_name.endswith(tuple(image_extension)):
                        if subfolder == 'E1':
                            if (image_name.endswith('1.png') or opposite_images.endswith('1.png')) and response_key == 'e':
                                correct_responses.append(row)
                            elif (image_name.endswith('2.png') or image_name.endswith('0.png') or opposite_images.endswith('2.png') or opposite_images.endswith('0.png')) and response_key == 'd':
                                correct_responses.append(row)
                            else:
                                incorrect_responses.append(row)
                        elif subfolder == 'E2':
                            if (image_name.endswith('2.png') or opposite_images.endswith('2.png')) and response_key == 'e':
                                correct_responses.append(row)
                            elif (image_name.endswith('0.png') or image_name.endswith('1.png') or opposite_images.endswith('1.png') or opposite_images.endswith('0.png')) and response_key == 'd':
                                correct_responses.append(row)
                            else:
                                incorrect_responses.append(row)


            # Write correct_responses.csv
            with open(os.path.join(subfolder_path, 'correct_responses.csv'), 'w', newline='') as csv_file:
                header.append('t1')
                csv_writer = csv.writer(csv_file)
                csv_writer.writerow(header)

                for i, row in enumerate(correct_responses):
                    row.append('t' + str(i + 1))
                    csv_writer.writerow(row)

            # Write incorrect_responses.csv
            with open(os.path.join(subfolder_path, 'incorrect_responses.csv'), 'w', newline='') as csv_file:
                header.append('t1')
                csv_writer = csv.writer(csv_file)
                csv_writer.writerow(header)

                for i, row in enumerate(incorrect_responses):
                    row.append('t' + str(i + 1))
                    csv_writer.writerow(row)


folder_path = '/content/drive/MyDrive/Research/Spatial_attnetion&Emotion/Data/Pilot_data_main(sixth)'
folders = os.listdir(folder_path)

for folder_name in folders:
    full_path = os.path.join(folder_path, folder_name)

    # Check if it's a directory and not named "ANK" or "AYU"
    if os.path.isdir(full_path) and folder_name not in ['XXX']:
        process_responses(folder_path, folder_name)

########################## Divide correct responses into target and distarctor ################################################
#################################################################################################################
#########################################################################################################################


# Define the GDrive path
gdrive_path = '/content/drive/MyDrive/Research/Spatial_attnetion&Emotion/Data/Pilot_data_main(ninth)'

# Iterate over the participant folders (e.g., 'ADA', 'ADB', etc.)
participant_folders = os.listdir(gdrive_path)
for participant_folder in participant_folders:
    participant_folder_path = os.path.join(gdrive_path, participant_folder)

    # Check if the participant folder exists
    if not os.path.exists(participant_folder_path) or not os.path.isdir(participant_folder_path):
        print(f"Participant folder '{participant_folder}' not found.")
        continue

    print(f"Processing participant folder: {participant_folder_path}")

    # Iterate over the E1, E2, E3 folders
    e_folders = ['E1', 'E2']
    for e_folder in e_folders:
        e_folder_path = os.path.join(participant_folder_path, e_folder)

        # Check if the E folder exists
        if not os.path.exists(e_folder_path) or not os.path.isdir(e_folder_path):
            print(f"E folder '{e_folder}' not found in participant folder '{participant_folder}'.")
            continue

        print(f"Processing E folder: {e_folder_path}")



        # Create the target and distractor CSV files
        target_file_path = os.path.join(e_folder_path, 'target.csv')
        print("target file created")
        distractor_file_path = os.path.join(e_folder_path, 'distractor.csv')

        target_file = open(target_file_path, 'w', newline='')
        distractor_file = open(distractor_file_path, 'w', newline='')
        target_writer = csv.writer(target_file)
        distractor_writer = csv.writer(distractor_file)

        # Write column names to the target and distractor CSV files
        target_column_names = ['image_name', 'response_key','location','cue_location','opposite_image']
        distractor_column_names = ['image_name', 'response_key','location','cue_location','opposite_images']

        # Read the correct_responses.csv file to determine the number of pupil data points
        correct_responses_file_path = os.path.join(e_folder_path, 'raw_data_pupil.csv')
        with open(correct_responses_file_path, 'r') as csv_file:
            reader = csv.reader(csv_file)
            header = next(reader)  # Read the header row
            num_points = len(header) - 5  # Subtract 3 for 'image_name', 'response_key', 'response_time'

        target_column_names += [f"t{i+1}" for i in range(num_points)]
        distractor_column_names += [f"t{i+1}" for i in range(num_points)]

        target_writer.writerow(target_column_names)
        distractor_writer.writerow(distractor_column_names)

        # Create a dictionary to store the rows for each image
        target_rows = {}
        distractor_rows = {}

        # Iterate over the files in the E folder
        for file_name in os.listdir(e_folder_path):
            file_path = os.path.join(e_folder_path, file_name)

            # Check if the file is 'correct_responses.csv'
            if file_name == 'raw_data_pupil.csv':
                with open(file_path, 'r') as csv_file:
                    reader = csv.reader(csv_file)
                    header = next(reader)  # Read the header row

                    # Find the column index of 'image_name', 'response_key', 'response_time'
                    image_name_index = header.index('image_name')
                    response_key_index = header.index('response_key')
                   # response_time_index = header.index('response_time')
                    location_index=header.index("location")
                    cue_location_index=header.index("cue_location")
                    opposite_image_index=header.index('opposite_images')

                    # Iterate over the rows and store them in the respective dictionaries
                    for row in reader:
                        image_name = row[image_name_index]
                        response_key = row[response_key_index]
                        #response_time = row[response_time_index]
                        location=row[location_index]
                        cue=row[cue_location_index]
                        opposite_images=row[opposite_image_index]
                        pupil_data = row[5:]  # Extract all pupil data points starting from the 4th column

                        if (e_folder == 'E1' and image_name.endswith('1.png')) or \
                           (e_folder == 'E2' and image_name.endswith('2.png')):
                            if image_name not in target_rows:
                                target_rows[image_name] = []
                            target_rows[image_name].append([image_name, response_key,location,cue,opposite_images] + pupil_data)
                        else:
                            if image_name not in distractor_rows:
                                distractor_rows[image_name] = []
                            distractor_rows[image_name].append([image_name, response_key,location,cue,opposite_images] + pupil_data)

        # Write the rows from the dictionaries to target and distractor CSV files
        for image_name, rows in target_rows.items():
            target_writer.writerows(rows)
        for image_name, rows in distractor_rows.items():
            distractor_writer.writerows(rows)

        # Close the CSV files
        target_file.close()
        distractor_file.close()

import os
import csv

# Define the GDrive path
gdrive_path = '/content/drive/MyDrive/Research/Spatial_attnetion&Emotion/Data/Pilot_data_main(ninth)'

# Iterate over the participant folders (e.g., 'ADA', 'ADB', etc.)
participant_folders = os.listdir(gdrive_path)
for participant_folder in participant_folders:
    participant_folder_path = os.path.join(gdrive_path, participant_folder)

    # Check if the participant folder exists
    if not os.path.exists(participant_folder_path) or not os.path.isdir(participant_folder_path):
        print(f"Participant folder '{participant_folder}' not found.")
        continue

    print(f"Processing participant folder: {participant_folder_path}")

    # Iterate over the E1, E2 folders
    e_folders = ['E1', 'E2']
    for e_folder in e_folders:
        e_folder_path = os.path.join(participant_folder_path, e_folder)

        # Check if the E folder exists
        if not os.path.exists(e_folder_path) or not os.path.isdir(e_folder_path):
            print(f"E folder '{e_folder}' not found in participant folder '{participant_folder}'.")
            continue

        print(f"Processing E folder: {e_folder_path}")

        # Create the target_cued and target_uncued CSV files
        target_cued_file_path = os.path.join(e_folder_path, 'target_cued.csv')
        target_uncued_file_path = os.path.join(e_folder_path, 'target_uncued.csv')

        with open(target_cued_file_path, 'w', newline='') as target_cued_file, \
             open(target_uncued_file_path, 'w', newline='') as target_uncued_file:

            target_cued_writer = csv.writer(target_cued_file)
            target_uncued_writer = csv.writer(target_uncued_file)

            # Write column names to the target_cued and target_uncued CSV files
            with open(os.path.join(e_folder_path, 'target.csv'), 'r') as csv_file:
                reader = csv.reader(csv_file)
                header = next(reader)  # Read the header row
                num_points = len(header) - 5  # Subtract 5 for 'image_name', 'response_key', 'response_time', 'location', 'cue_location'

            target_cued_column_names = ['image_name', 'response_key', 'location', 'cue_location'] + [f"t{i+1}" for i in range(num_points)]
            target_uncued_column_names = ['image_name', 'response_key', 'location', 'cue_location'] + [f"t{i+1}" for i in range(num_points)]

            target_cued_writer.writerow(target_cued_column_names)
            target_uncued_writer.writerow(target_uncued_column_names)

            # Iterate over the files in the E folder
            for file_name in os.listdir(e_folder_path):
                file_path = os.path.join(e_folder_path, file_name)

                # Check if the file is 'target.csv'
                if file_name == 'target.csv':
                    with open(file_path, 'r') as csv_file:
                        reader = csv.reader(csv_file)
                        header = next(reader)  # Read the header row

                        # Find the column index of 'image_name', 'response_key', 'response_time', 'location', 'cue_location'
                        image_name_index = header.index('image_name')
                        response_key_index = header.index('response_key')
                        #response_time_index = header.index('response_time')
                        location_index = header.index('location')
                        cue_location_index = header.index('cue_location')

                        # Iterate over the rows and store them in the respective CSV files
                        for row in reader:
                            image_name = row[image_name_index]
                            response_key = row[response_key_index]
                            #response_time = row[response_time_index]
                            location = row[location_index]
                            cue_location = row[cue_location_index]
                            pupil_data = row[5:]  # Extract all pupil data points starting from the 6th column

                            if location == cue_location:
                                target_cued_writer.writerow([image_name, response_key, location, cue_location] + pupil_data)
                            else:
                                target_uncued_writer.writerow([image_name, response_key, location, cue_location] + pupil_data)

print("________________________________________________________Processing complete for cued and uncued ________________________________________________________")

"""## Analysis with left/right and all degree"""

# Path to the parent folder


parent_folder_path = '/content/drive/MyDrive/Research/Spatial_attnetion&Emotion/Data/Pilot_data_main(fifth)'

# Get the list of folders inside the parent folder
folder_names = os.listdir(parent_folder_path)

excluded_folders=["xxx"]




# Create an empty dictionary to store the average values arrays for each folder
average_values_dict = {}

# Iterate over the folder names
for folder_name in folder_names:
    # Check if the folder_name is in the excluded_folders list
    if folder_name in excluded_folders:
        continue  # Skip this iteration and move to the next folder

    # Path to the folder
    folder_path = os.path.join(parent_folder_path, folder_name)


    # Check if the path is a directory
    if os.path.isdir(folder_path):
        # Create empty arrays to store the average values for E1, E2, and E3
        average_values_E1_6 = []
        average_values_E1_12=[]
        average_values_E1_m6=[]
        average_values_E1_m12=[]

        average_values_E2_6 = []
        average_values_E2_12=[]
        average_values_E2_m6=[]
        average_values_E2_m12=[]

        # Iterate over E1, E2, and E3
        for experiment in ['E1', 'E2']:
            # Path to the target.csv file
            file_path = os.path.join(folder_path, experiment, 'target_cued.csv')

            try:
                # Read the CSV file, skipping the first row
                data = pd.read_csv(file_path, skiprows=1, header=None)

                # Drop the last column
                data = data.iloc[:, :-1]
                data[3] = data[3].astype(int)


                ################# Divide data into 4 Groups ######################################################

                # Create four separate DataFrames based on the 'location' column
                data_6 = data[data[3] == 8]
                data_12 = data[data[3] == 16]
                data_m6 = data[data[3] == -8]
                data_m12 = data[data[3] == -16]



                ############################################################################################

                # Exclude images with response times outside the range of 0.200 to 0.900
                valid_images_6 = data_6[(data_6.iloc[:, 2] >= 0.200) & (data_6.iloc[:, 2] <=1.5)]
                valid_images_12= data_12[(data_12.iloc[:, 2] >= 0.200) & (data_12.iloc[:, 2] <=1.5)]
                valid_images_m12= data_m12[(data_m12.iloc[:, 2] >= 0.200) & (data_m12.iloc[:, 2] <=1.5)]
                valid_images_m6 = data_m6[(data_m6.iloc[:, 2] >= 0.200) & (data_m6.iloc[:, 2] <=1.5)]


                #valid_images=data

                # Ignore the first 13 pupil data
                ignored_columns_6 = valid_images_6.columns[3:16]
                ignored_columns_12 = valid_images_12.columns[3:16]
                ignored_columns_m6 = valid_images_m6.columns[3:16]
                ignored_columns_m12 = valid_images_m12.columns[3:16]


                # Take the next 25 pupil data and calculate the mean, excluding non-numeric columns
                #baseline_data = valid_images.iloc[:, 15:40].apply(
                    #lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                #)

                # New baseline:Take the next 25 pupil data and calculate the mean, excluding non-numeric columns
                baseline_data_6 = valid_images_6.iloc[:, 1:81].apply(
                    lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                )
                baseline_data_12 = valid_images_12.iloc[:, 1:81].apply(
                    lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                )
                baseline_data_m6 = valid_images_m6.iloc[:, 1:81].apply(
                    lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                )
                baseline_data_m12 = valid_images_m12.iloc[:, 1:81].apply(
                    lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                )


                print("++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
                print("Baseline data is",data_6)


                # Subtract the mean from all the pupil values in each row, excluding non-numeric columns
                #baseline_corrected_data = valid_images.iloc[:, 3:].apply(lambda row: pd.to_numeric(row, errors='coerce') - baseline_data, axis=0)

                ##################################### Divisive baseline ################################

                  # Subtract the mean from all the pupil values in each row, excluding non-numeric columns
                baseline_corrected_data_6 = valid_images_6.iloc[:, 6:].apply(
                    lambda row: pd.to_numeric(row, errors='coerce') / baseline_data_6, axis=0
                )
                baseline_corrected_data_12 = valid_images_12.iloc[:, 6:].apply(
                    lambda row: pd.to_numeric(row, errors='coerce') / baseline_data_12, axis=0
                )
                baseline_corrected_data_m6 = valid_images_m6.iloc[:, 6:].apply(
                    lambda row: pd.to_numeric(row, errors='coerce') / baseline_data_m6, axis=0
                )
                baseline_corrected_data_m12 = valid_images_m12.iloc[:, 6:].apply(
                    lambda row: pd.to_numeric(row, errors='coerce') / baseline_data_m12, axis=0
                )

                ############################################################### ################################
                # Calculate Z-score normalization for each image row-wise, excluding non-numeric columns
                #zscore_normalized_data = baseline_corrected_data.apply(lambda row: (row - row.mean()) / row.std(), axis=1)


                ######################### Without Z score normalization ###########
                zscore_normalized_data_6=baseline_corrected_data_6
                zscore_normalized_data_12=baseline_corrected_data_12
                zscore_normalized_data_m6=baseline_corrected_data_m6
                zscore_normalized_data_m12=baseline_corrected_data_m12


                # Replace NaN values with zeros
                zscore_normalized_data_6 = zscore_normalized_data_6.fillna(0)
                zscore_normalized_data_m6 = zscore_normalized_data_m6.fillna(0)
                zscore_normalized_data_12 = zscore_normalized_data_12.fillna(0)
                zscore_normalized_data_m12 = zscore_normalized_data_m12.fillna(0)


                # Find the length of the longest pupil data array (considering non-empty cells)
                longest_length_6 = zscore_normalized_data_6.apply(
                    lambda row: len(row[row != 0]), axis=1
                ).max()

                longest_length_12 = zscore_normalized_data_12.apply(
                    lambda row: len(row[row != 0]), axis=1
                ).max()

                longest_length_m6 = zscore_normalized_data_m6.apply(
                    lambda row: len(row[row != 0]), axis=1
                ).max()

                longest_length_m12 = zscore_normalized_data_m12.apply(
                    lambda row: len(row[row != 0]), axis=1
                ).max()

                # Check if longest_length is NaN, assign 0 as default value
                if pd.isnull(longest_length_6):
                    longest_length_6 = 0
                if pd.isnull(longest_length_m6):
                    longest_length_m6 = 0
                if pd.isnull(longest_length_12):
                    longest_length_12 = 0
                if pd.isnull(longest_length_m12):
                    longest_length_m12 = 0


                # Convert the longest_length to an integer
                longest_length_6 = int(longest_length_6)
                longest_length_12 = int(longest_length_12)
                longest_length_m6 = int(longest_length_m6)
                longest_length_m12 = int(longest_length_m12)


                # Truncate all images to the length of the longest pupil data array and fill empty cells with zeros
                zscore_normalized_data_6_truncated = zscore_normalized_data_6.apply(
                    lambda row: np.pad(row[:longest_length_6], (0, longest_length_6 - len(row[:longest_length_6])), mode='constant', constant_values=0), axis=1
                )
                zscore_normalized_data_m6_truncated = zscore_normalized_data_m6.apply(
                    lambda row: np.pad(row[:longest_length_m6], (0, longest_length_m6 - len(row[:longest_length_m6])), mode='constant', constant_values=0), axis=1
                )
                zscore_normalized_data_12_truncated = zscore_normalized_data_12.apply(
                    lambda row: np.pad(row[:longest_length_12], (0, longest_length_12 - len(row[:longest_length_12])), mode='constant', constant_values=0), axis=1
                )
                zscore_normalized_data_m12_truncated = zscore_normalized_data_m12.apply(
                    lambda row: np.pad(row[:longest_length_m12], (0, longest_length_m12 - len(row[:longest_length_m12])), mode='constant', constant_values=0), axis=1
                )

                # Truncate the dataset to the length of the longest pupil data array
                zscore_normalized_data_6_truncated = zscore_normalized_data_6_truncated.apply(
                    lambda row: row[:longest_length_6]
                )
                zscore_normalized_data_12_truncated = zscore_normalized_data_12_truncated.apply(
                    lambda row: row[:longest_length_12]
                )
                zscore_normalized_data_m6_truncated = zscore_normalized_data_m6_truncated.apply(
                    lambda row: row[:longest_length_m6]
                )
                zscore_normalized_data_m12_truncated = zscore_normalized_data_m12_truncated.apply(
                    lambda row: row[:longest_length_m12]
                )


                # Create a new DataFrame to store the preprocessed data
                preprocessed_data_6= pd.DataFrame()
                preprocessed_data_12= pd.DataFrame()
                preprocessed_data_m6= pd.DataFrame()
                preprocessed_data_m12= pd.DataFrame()


                # Add the image name, response key, and response time columns to the new DataFrame
                preprocessed_data_6['Image Name'] = valid_images_6.iloc[:, 0]
                preprocessed_data_6['Response Key'] = valid_images_6.iloc[:, 1]
                preprocessed_data_6['Response Time'] = valid_images_6.iloc[:, 2]

                preprocessed_data_m6['Image Name'] = valid_images_m6.iloc[:, 0]
                preprocessed_data_m6['Response Key'] = valid_images_m6.iloc[:, 1]
                preprocessed_data_m6['Response Time'] = valid_images_m6.iloc[:, 2]

                preprocessed_data_12['Image Name'] = valid_images_12.iloc[:, 0]
                preprocessed_data_12['Response Key'] = valid_images_12.iloc[:, 1]
                preprocessed_data_12['Response Time'] = valid_images_12.iloc[:, 2]

                preprocessed_data_m12['Image Name'] = valid_images_m12.iloc[:, 0]
                preprocessed_data_m12['Response Key'] = valid_images_m12.iloc[:, 1]
                preprocessed_data_m12['Response Time'] = valid_images_m12.iloc[:, 2]

                # Add the preprocessed data to the new DataFrame
                for i in range(longest_length_6):
                    preprocessed_data_6[f'Pupil Data {i+1}'] = zscore_normalized_data_6_truncated.apply(lambda row: row[i])
                for i in range(longest_length_12):
                    preprocessed_data_12[f'Pupil Data {i+1}'] = zscore_normalized_data_12_truncated.apply(lambda row: row[i])
                for i in range(longest_length_m6):
                    preprocessed_data_m6[f'Pupil Data {i+1}'] = zscore_normalized_data_m6_truncated.apply(lambda row: row[i])
                for i in range(longest_length_m12):
                    preprocessed_data_m12[f'Pupil Data {i+1}'] = zscore_normalized_data_m12_truncated.apply(lambda row: row[i])




                # Calculate the average value for each column
                average_values_6 = preprocessed_data_6.iloc[:, 6:].apply(lambda col: col.sum() / col[col != 0].count(), axis=0)
                average_value_6=average_values_6[1:275]

                average_values_12 = preprocessed_data_12.iloc[:, 6:].apply(lambda col: col.sum() / col[col != 0].count(), axis=0)
                average_values_12=average_values_12[1:275]

                average_values_m6 = preprocessed_data_m6.iloc[:, 6:].apply(lambda col: col.sum() / col[col != 0].count(), axis=0)
                average_values_m6=average_values_m6[1:275]

                average_values_m12 = preprocessed_data_m12.iloc[:, 6:].apply(lambda col: col.sum() / col[col != 0].count(), axis=0)
                average_values_m12=average_values_m12[1:275]


                # Convert the average values to an array
                average_values_array_6 = average_values_6.to_numpy()
                average_values_array_m6 = average_values_m6.to_numpy()
                average_values_array_12 = average_values_12.to_numpy()
                average_values_array_m12 = average_values_m12.to_numpy()
#############################################################################################################################################################################################################


                # Store the average values array in the corresponding E1, E2, or E3 array
                if experiment == 'E1':
                    average_values_E1_6 = average_values_array_6
                    average_values_E1_12 = average_values_array_12
                    average_values_E1_m6 = average_values_array_m6
                    average_values_E1_m12 = average_values_array_m12
                elif experiment == 'E2':
                    average_values_E2_6 = average_values_array_6
                    average_values_E2_12 = average_values_array_12
                    average_values_E2_m6 = average_values_array_m6
                    average_values_E2_m12 = average_values_array_m12


            except pd.errors.EmptyDataError:
                # Handle the case when the CSV file is empty
                print(f"The CSV file {file_path} is empty.")

        # Store the average values arrays for E1, E2, and E3 in the dictionary
        average_values_dict[folder_name] = {
            'E1_6': average_values_E1_6,
            'E1_12': average_values_E1_12,
            'E1_m6': average_values_E1_m6,
            'E1_m12': average_values_E1_m12,
            'E2_6': average_values_E2_6,
            'E2_12': average_values_E2_12,
            'E2_m6': average_values_E2_m6,
            'E2_m12': average_values_E2_m12,

        }

#### Participant wise plot

import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
import numpy as np

# Iterate over the dictionary containing the average values arrays for each participant
for participant, values in average_values_dict.items():
    try:
        # Get the average values arrays for E1, E2, and E3
        average_values_array_E1_6 = values['E1_6']
        average_values_array_E2_6 = values['E2_6']
        average_values_array_E1_m6 = values['E1_m6']
        average_values_array_E2_m6 = values['E2_m6']
        average_values_array_E1_12 = values['E1_12']
        average_values_array_E2_12 = values['E2_12']
        average_values_array_E1_m12 = values['E1_m12']
        average_values_array_E2_m12 = values['E2_m12']



        # Determine the minimum length among the arrays
        min_length_6 = min(len(average_values_array_E1_6), len(average_values_array_E2_6))
        min_length_12 = min(len(average_values_array_E1_12), len(average_values_array_E2_12))
        min_length_m6 = min(len(average_values_array_E1_m6), len(average_values_array_E2_m6))
        min_length_m12 = min(len(average_values_array_E1_m12), len(average_values_array_E2_m12))
        # Truncate the arrays to the minimum length
        average_values_array_E1_6 = average_values_array_E1_6[:min_length_6]
        average_values_array_E2_6 = average_values_array_E2_6[:min_length_6]
        average_values_array_E1_12 = average_values_array_E1_12[:min_length_12]
        average_values_array_E2_12 = average_values_array_E2_12[:min_length_12]
        average_values_array_E1_m6 = average_values_array_E1_m6[:min_length_m6]
        average_values_array_E2_m6 = average_values_array_E2_m6[:min_length_m6]
        average_values_array_E1_m12 = average_values_array_E1_m12[:min_length_m12]
        average_values_array_E2_m12 = average_values_array_E2_m12[:min_length_m12]



        # Apply Savitzky-Golay filter to the arrays
        window_length_6 = 11 # Adjust the window length as desired
        filtered_E1_6 = savgol_filter(average_values_array_E1_6, window_length=window_length_6, polyorder=2)
        filtered_E2_6 = savgol_filter(average_values_array_E2_6, window_length=window_length_6, polyorder=2)

        window_length_m6 =11 # Adjust the window length as desired
        filtered_E1_m6 = savgol_filter(average_values_array_E1_m6, window_length=window_length_m6, polyorder=2)
        filtered_E2_m6 = savgol_filter(average_values_array_E2_m6, window_length=window_length_m6, polyorder=2)

        window_length_12 =11 # Adjust the window length as desired
        filtered_E1_12 = savgol_filter(average_values_array_E1_12, window_length=window_length_12, polyorder=2)
        filtered_E2_12 = savgol_filter(average_values_array_E2_12, window_length=window_length_12, polyorder=2)

        window_length_m12 =11 # Adjust the window length as desired
        filtered_E1_m12 = savgol_filter(average_values_array_E1_m12, window_length=window_length_m12, polyorder=2)
        filtered_E2_m12 = savgol_filter(average_values_array_E2_m12, window_length=window_length_m12, polyorder=2)




        # Calculate time axis values in ms
        time_axis_6 = (np.arange(min_length_6) * 8)-650
        time_axis_m6 = (np.arange(min_length_m6) * 8)-650
        time_axis_12 = (np.arange(min_length_12) * 8)-650
        time_axis_m12 = (np.arange(min_length_m12) * 8)-650

        # Plot the filtered data
        plt.figure(figsize=(20, 10))
        plt.xlim(-650,1500)

        plt.plot(time_axis_6, filtered_E1_6, label='Happy faces 6',linewidth=2,color='blue')
        plt.plot(time_axis_6, filtered_E2_6, label='Angry faces 6',linewidth=2,color='orange')
        plt.plot(time_axis_m6, filtered_E1_m6, label='Happy faces m6',linewidth=2,color='skyblue')
        plt.plot(time_axis_m6, filtered_E2_m6, label='Angry faces m6',linewidth=2,color='red')
        plt.plot(time_axis_12, filtered_E1_12, label='Happy faces 12',linewidth=2,color='blue',linestyle='--')
        plt.plot(time_axis_12, filtered_E2_12, label='Angry faces 12',linewidth=2,color='orange',linestyle='--')
        plt.plot(time_axis_m12, filtered_E1_m12, label='Happy faces m12',linewidth=2,color='skyblue',linestyle='--')
        plt.plot(time_axis_m12, filtered_E2_m12, label='Angry faces m12',linewidth=2,color='red',linestyle='--')


        plt.axvline(x=0, color='red', linestyle='--', label='Cue start')
        plt.axvline(x=150, color='black', linestyle='--', label='Stim start')
        plt.axvline(x=350, color='blue', linestyle='--', label='Stim stop')


        plt.xlabel('Time (ms)')
        plt.ylabel('Filtered Data')
        plt.title(f'Participant {participant}: Mean Pupillary Response')
        plt.legend()
        plt.grid(True)
        plt.show()
    except ValueError as e:
        print(f"Error for Participant {participant}: {str(e)}")

"""## **Plot participant wise data if we don't need left/right**"""

# Path to the parent folder


parent_folder_path = '/content/drive/MyDrive/Research/Spatial_attnetion&Emotion/Data/Pilot_data_main(ninth)'

# Get the list of folders inside the parent folder
folder_names = os.listdir(parent_folder_path)

excluded_folders=["xxx"]




# Create an empty dictionary to store the average values arrays for each folder
average_values_dict = {}

# Iterate over the folder names
for folder_name in folder_names:
    # Check if the folder_name is in the excluded_folders list
    if folder_name in excluded_folders:
        continue  # Skip this iteration and move to the next folder

    # Path to the folder
    folder_path = os.path.join(parent_folder_path, folder_name)


    # Check if the path is a directory
    if os.path.isdir(folder_path):
        # Create empty arrays to store the average values for E1, E2, and E3
        average_values_E1_6 = []
        average_values_E1_12=[]

        average_values_E2_6 = []
        average_values_E2_12=[]

        # Iterate over E1, E2, and E3
        for experiment in ['E1', 'E2']:
            # Path to the target.csv file
            file_path = os.path.join(folder_path, experiment, 'target_cued.csv')

            try:
                # Read the CSV file, skipping the first row
                data = pd.read_csv(file_path, skiprows=1, header=None)

                # Drop the last column
                data = data.iloc[:, :-1]
                data[3] = data[3].astype(int)


                ################# Divide data into 4 Groups ######################################################

                # Create four separate DataFrames based on the 'location' column
                data_6 = data[(data[2] == 7) | (data[2] == -7)]
                data_12 = data[(data[2] == 14) | (data[2] == -14)]




                ############################################################################################

                # Exclude images with response times outside the range of 0.200 to 0.900
                #valid_images_6 = data_6[(data_6.iloc[:, 2] >= 0.200) & (data_6.iloc[:, 2] <=1.5)]
                #valid_images_12= data_12[(data_12.iloc[:, 2] >= 0.200) & (data_12.iloc[:, 2] <=1.5)]

                valid_images_6 = data_6
                valid_images_12= data_12


                #valid_images=data

                # Ignore the first 13 pupil data
                ignored_columns_6 = valid_images_6.columns[3:16]
                ignored_columns_12 = valid_images_12.columns[3:16]


                # Take the next 25 pupil data and calculate the mean, excluding non-numeric columns
                #baseline_data = valid_images.iloc[:, 15:40].apply(
                    #lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                #)

                # New baseline:Take the next 25 pupil data and calculate the mean, excluding non-numeric columns
                baseline_data_6 = valid_images_6.iloc[:, 1:81].apply(
                    lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                )
                baseline_data_12 = valid_images_12.iloc[:, 1:81].apply(
                    lambda row: pd.to_numeric(row, errors='coerce').mean(), axis=1
                )



                print("++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
                print("Baseline data is",data_6)


                # Subtract the mean from all the pupil values in each row, excluding non-numeric columns
                #baseline_corrected_data = valid_images.iloc[:, 3:].apply(lambda row: pd.to_numeric(row, errors='coerce') - baseline_data, axis=0)

                ##################################### Divisive baseline ################################

                  # Subtract the mean from all the pupil values in each row, excluding non-numeric columns
                baseline_corrected_data_6 = valid_images_6.iloc[:, 6:].apply(
                    lambda row: pd.to_numeric(row, errors='coerce') / baseline_data_6, axis=0
                )
                baseline_corrected_data_12 = valid_images_12.iloc[:, 6:].apply(
                    lambda row: pd.to_numeric(row, errors='coerce') / baseline_data_12, axis=0
                )

                ############################################################### ################################
                # Calculate Z-score normalization for each image row-wise, excluding non-numeric columns
                #zscore_normalized_data = baseline_corrected_data.apply(lambda row: (row - row.mean()) / row.std(), axis=1)


                ######################### Without Z score normalization ###########
                zscore_normalized_data_6=baseline_corrected_data_6
                zscore_normalized_data_12=baseline_corrected_data_12



                # Replace NaN values with zeros
                zscore_normalized_data_6 = zscore_normalized_data_6.fillna(0)
                zscore_normalized_data_12 = zscore_normalized_data_12.fillna(0)



                # Find the length of the longest pupil data array (considering non-empty cells)
                longest_length_6 = zscore_normalized_data_6.apply(
                    lambda row: len(row[row != 0]), axis=1
                ).max()

                longest_length_12 = zscore_normalized_data_12.apply(
                    lambda row: len(row[row != 0]), axis=1
                ).max()




                # Check if longest_length is NaN, assign 0 as default value
                if pd.isnull(longest_length_6):
                    longest_length_6 = 0

                if pd.isnull(longest_length_12):
                    longest_length_12 = 0



                # Convert the longest_length to an integer
                longest_length_6 = int(longest_length_6)
                longest_length_12 = int(longest_length_12)


                # Truncate all images to the length of the longest pupil data array and fill empty cells with zeros
                zscore_normalized_data_6_truncated = zscore_normalized_data_6.apply(
                    lambda row: np.pad(row[:longest_length_6], (0, longest_length_6 - len(row[:longest_length_6])), mode='constant', constant_values=0), axis=1
                )

                zscore_normalized_data_12_truncated = zscore_normalized_data_12.apply(
                    lambda row: np.pad(row[:longest_length_12], (0, longest_length_12 - len(row[:longest_length_12])), mode='constant', constant_values=0), axis=1
                )

                # Truncate the dataset to the length of the longest pupil data array
                zscore_normalized_data_6_truncated = zscore_normalized_data_6_truncated.apply(
                    lambda row: row[:longest_length_6]
                )
                zscore_normalized_data_12_truncated = zscore_normalized_data_12_truncated.apply(
                    lambda row: row[:longest_length_12]
                )


                # Create a new DataFrame to store the preprocessed data
                preprocessed_data_6= pd.DataFrame()
                preprocessed_data_12= pd.DataFrame()



                # Add the image name, response key, and response time columns to the new DataFrame
                preprocessed_data_6['Image Name'] = valid_images_6.iloc[:, 0]
                preprocessed_data_6['Response Key'] = valid_images_6.iloc[:, 1]
                preprocessed_data_6['Response Time'] = valid_images_6.iloc[:, 2]



                preprocessed_data_12['Image Name'] = valid_images_12.iloc[:, 0]
                preprocessed_data_12['Response Key'] = valid_images_12.iloc[:, 1]
                preprocessed_data_12['Response Time'] = valid_images_12.iloc[:, 2]



                # Add the preprocessed data to the new DataFrame
                for i in range(longest_length_6):
                    preprocessed_data_6[f'Pupil Data {i+1}'] = zscore_normalized_data_6_truncated.apply(lambda row: row[i])
                for i in range(longest_length_12):
                    preprocessed_data_12[f'Pupil Data {i+1}'] = zscore_normalized_data_12_truncated.apply(lambda row: row[i])





                # Calculate the average value for each column
                average_values_6 = preprocessed_data_6.iloc[:, 6:].apply(lambda col: col.sum() / col[col != 0].count(), axis=0)
                average_value_6=average_values_6[1:275]

                average_values_12 = preprocessed_data_12.iloc[:, 6:].apply(lambda col: col.sum() / col[col != 0].count(), axis=0)
                average_values_12=average_values_12[1:275]



                # Convert the average values to an array
                average_values_array_6 = average_values_6.to_numpy()
                average_values_array_12 = average_values_12.to_numpy()
#############################################################################################################################################################################################################


                # Store the average values array in the corresponding E1, E2, or E3 array
                if experiment == 'E1':
                    average_values_E1_6 = average_values_array_6
                    average_values_E1_12 = average_values_array_12

                elif experiment == 'E2':
                    average_values_E2_6 = average_values_array_6
                    average_values_E2_12 = average_values_array_12



            except pd.errors.EmptyDataError:
                # Handle the case when the CSV file is empty
                print(f"The CSV file {file_path} is empty.")

        # Store the average values arrays for E1, E2, and E3 in the dictionary
        average_values_dict[folder_name] = {
            'E1_6': average_values_E1_6,
            'E1_12': average_values_E1_12,

            'E2_6': average_values_E2_6,
            'E2_12': average_values_E2_12,

        }

average_values_dict

#### Participant wise plot

import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
import numpy as np

# Iterate over the dictionary containing the average values arrays for each participant
for participant, values in average_values_dict.items():
    try:
        # Get the average values arrays for E1, E2, and E3
        average_values_array_E1_6 = values['E1_6']
        average_values_array_E2_6 = values['E2_6']

        average_values_array_E1_12 = values['E1_12']
        average_values_array_E2_12 = values['E2_12']






        # Determine the minimum length among the arrays
        min_length_6 = min(len(average_values_array_E1_6), len(average_values_array_E2_6))
        min_length_12 = min(len(average_values_array_E1_12), len(average_values_array_E2_12))

        # Truncate the arrays to the minimum length
        average_values_array_E1_6 = average_values_array_E1_6[:min_length_6]
        average_values_array_E2_6 = average_values_array_E2_6[:min_length_6]
        average_values_array_E1_12 = average_values_array_E1_12[:min_length_12]
        average_values_array_E2_12 = average_values_array_E2_12[:min_length_12]




        # Apply Savitzky-Golay filter to the arrays
        window_length_6 = 11 # Adjust the window length as desired
        filtered_E1_6 = savgol_filter(average_values_array_E1_6, window_length=window_length_6, polyorder=2)
        filtered_E2_6 = savgol_filter(average_values_array_E2_6, window_length=window_length_6, polyorder=2)


        window_length_12 =11 # Adjust the window length as desired
        filtered_E1_12 = savgol_filter(average_values_array_E1_12, window_length=window_length_12, polyorder=2)
        filtered_E2_12 = savgol_filter(average_values_array_E2_12, window_length=window_length_12, polyorder=2)






        # Calculate time axis values in ms
        time_axis_6 = (np.arange(min_length_6) * 8)-650
        time_axis_12 = (np.arange(min_length_12) * 8)-650


        # Plot the filtered data
        plt.figure(figsize=(20, 10))
        plt.xlim(-650,1500)

        plt.plot(time_axis_6, filtered_E1_6, label='Happy faces 6',linewidth=2,color='blue')
        plt.plot(time_axis_6, filtered_E2_6, label='Angry faces 6',linewidth=2,color='orange')
        plt.plot(time_axis_12, filtered_E1_12, label='Happy faces 12',linewidth=2,color='blue',linestyle='--')
        plt.plot(time_axis_12, filtered_E2_12, label='Angry faces 12',linewidth=2,color='orange',linestyle='--')


        plt.axvline(x=0, color='red', linestyle='--', label='Cue start')
        plt.axvline(x=150, color='black', linestyle='--', label='cue stop')
        plt.axvline(x=650, color='blue', linestyle='--', label='Stim start')
        plt.axvline(x=800, color='green', linestyle='--', label='Stim stop')



        plt.xlabel('Time (ms)')
        plt.ylabel('Filtered Data')
        plt.title(f'Participant {participant}: Mean Pupillary Response')
        plt.legend()
        plt.grid(True)
        plt.show()
    except ValueError as e:
        print(f"Error for Participant {participant}: {str(e)}")



"""## **Plot average data**"""

def calculate_column_averages(dataset_name, data_dict):
    # Find the minimum length of the given dataset across participants
    min_length = min(len(participant_data[dataset_name]) for participant_data in data_dict.values())

    # Initialize an empty list to store dataset values by column
    values_by_column = [[] for _ in range(min_length)]

    # Extract dataset values from each participant, slice to the minimum length, and store them column-wise
    for participant_data in data_dict.values():
        dataset_values = participant_data[dataset_name][:min_length]
        for column_index, value in enumerate(dataset_values):
            values_by_column[column_index].append(value)

    # Calculate column-wise averages for the dataset
    column_averages = [sum(column) / len(column) for column in values_by_column]

    return column_averages

# Example usage for E1
E1_avg_6 = calculate_column_averages('E1_6', average_values_dict)
E1_avg_12 = calculate_column_averages('E1_12', average_values_dict)

# Example usage for E2
E2_avg_6 = calculate_column_averages('E2_6', average_values_dict)
E2_avg_12 = calculate_column_averages('E2_12', average_values_dict)





# Example usage for E1
#E1_avg_red_happy = calculate_column_averages('E1', average_values_dict_dot)

# Example usage for E2
#E2_avg_red_angry = calculate_column_averages('E2', average_values_dict_dot)

# Example usage for E2
#E3_avg_red_neutral = calculate_column_averages('E3', average_values_dict_dot)


# Repeat for E3, E1_dis, E2_dis, and E3_dis as needed

#### Participant wise plot

import matplotlib.pyplot as plt
from scipy.signal import savgol_filter


# Get the average values arrays for E1, E2, and E3
average_values_array_E1_6 =E1_avg_6
average_values_array_E2_6 = E2_avg_6
average_values_array_E1_12 =E1_avg_12
average_values_array_E2_12 = E2_avg_12


#average_values_array_E1_red =E1_avg_red_happy
#average_values_array_E2_red = E2_avg_red_angry
#average_values_array_E3_red = E3_avg_red_neutral



# Determine the minimum length among the arrays
#min_length = min(len(average_values_array_E1), len(average_values_array_E2), len(average_values_array_E3),len(average_values_array_E1_red),len(average_values_array_E2_red),len(average_values_array_E3_red))
min_length_6 = min(len(average_values_array_E1_6), len(average_values_array_E2_6))
min_length_12 = min(len(average_values_array_E1_12), len(average_values_array_E2_12))

# Truncate the arrays to the minimum length
average_values_array_E1_6 = average_values_array_E1_6[:min_length_6]
average_values_array_E2_6 = average_values_array_E2_6[:min_length_6]
average_values_array_E1_12 = average_values_array_E1_12[:min_length_12]
average_values_array_E2_12 = average_values_array_E2_12[:min_length_12]

#average_values_array_E1_red = average_values_array_E1_red[:min_length]
#average_values_array_E2_red = average_values_array_E2_red[:min_length]
#average_values_array_E3_red = average_values_array_E3_red[:min_length]


# Apply Savitzky-Golay filter to the arrays
window_length =7  # Adjust the window length as desired
filtered_E1_6 = savgol_filter(average_values_array_E1_6, window_length=window_length, polyorder=2)
filtered_E2_6 = savgol_filter(average_values_array_E2_6, window_length=window_length, polyorder=2)
filtered_E1_12 = savgol_filter(average_values_array_E1_12, window_length=window_length, polyorder=2)
filtered_E2_12 = savgol_filter(average_values_array_E2_12, window_length=window_length, polyorder=2)

#filtered_E1_red = savgol_filter(average_values_array_E1_red, window_length=window_length, polyorder=2)
#filtered_E2_red = savgol_filter(average_values_array_E2_red, window_length=window_length, polyorder=2)
#filtered_E3_red = savgol_filter(average_values_array_E3_red, window_length=window_length, polyorder=2)



# Calculate time axis values in ms
time_axis_6 = (np.arange(min_length_6) * 8)-650
time_axis_12 = (np.arange(min_length_12) * 8)-650


# Plot the filtered data
plt.figure(figsize=(20, 10))
plt.xlim(-650,1500)

plt.plot(time_axis_6, filtered_E1_6, label='Happy 6',linewidth=2,color='blue')
plt.plot(time_axis_6, filtered_E2_6, label='Angry 6',linewidth=2,color='orange')
plt.plot(time_axis_12, filtered_E1_12, label='Happy 12',linewidth=2,color='blue',linestyle='--')
plt.plot(time_axis_12, filtered_E2_12, label='Angry 12',linewidth=2,color='orange',linestyle='--')






#plt.plot(time_axis, filtered_E1_red, label='Happy',linewidth=3,color='black',linestyle="--")
#plt.plot(time_axis, filtered_E2_red, label='Angry',linewidth=3,color='orange',linestyle="--")
#plt.plot(time_axis, filtered_E3_red, label=' Neutral',linewidth=3,color='green',linestyle="--")



plt.axvline(x=0, color='red', linestyle='--', label='Cue start')
plt.axvline(x=150, color='black', linestyle='--', label='Stim start')
plt.axvline(x=300, color='blue', linestyle='--', label='Stim stop')


plt.xlabel('Time (ms)')
plt.ylabel('Filtered Data')
plt.title('Mean Pupillary Response')
plt.legend()
plt.grid(True)
plt.show()

